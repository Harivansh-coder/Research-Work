{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qA3MaSfC1yf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Add, Concatenate, Layer, MultiHeadAttention, Dropout, LayerNormalization\n",
        "from tensorflow.keras.callbacks import History\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer, TFVisionEncoderDecoderModel\n",
        "\n",
        "# Function to evaluate the model using BLEU, METEOR, ROUGE, CIDEr\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "# Ensure nltk resources are downloaded\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# required packages\n",
        "# pip install tensorflow transformers nltk matplotlib pycocoevalcap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the captions file\n",
        "captions_file = './flickr8k/captions.txt'\n",
        "captions = pd.read_csv(captions_file)\n",
        "captions.columns = ['image', 'caption']\n",
        "\n",
        "# Display the first few rows of the captions file\n",
        "captions.head()"
      ],
      "metadata": {
        "id": "QM0kcTjMDKnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample image\n",
        "def display_image(image_path):\n",
        "    img = plt.imread(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "display_image('./flickr8k/Images/96420612_feb18fc6c6.jpg')"
      ],
      "metadata": {
        "id": "kZfkYTc5DRhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "def text_preprocessing(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    data['caption'] = data['caption'].astype(str).fillna('')\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
        "    data['caption'] = data['caption'].apply(lambda x: re.sub(r'[^a-z]', ' ', x))\n",
        "    data['caption'] = data['caption'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
        "    data['caption'] = data['caption'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words and len(word) > 1]))\n",
        "    data['caption'] = data['caption'].apply(lambda x: f\"startseq {x.strip()} endseq\")\n",
        "    return data\n",
        "\n",
        "captions = text_preprocessing(captions)\n",
        "captions.head()\n"
      ],
      "metadata": {
        "id": "FYNvzztBDRuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 7000 entries from the dataset\n",
        "# captions_sample = captions.sample(n=7000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Create a dictionary to map images to their captions\n",
        "captions_dict = {}\n",
        "for idx, row in captions.iterrows():\n",
        "    img, caption = row['image'], row['caption']\n",
        "    if img not in captions_dict:\n",
        "        captions_dict[img] = []\n",
        "    captions_dict[img].append(caption)\n",
        "\n",
        "print(f\"Number of images: {len(captions_dict)}\")\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_images, val_images = train_test_split(list(captions_dict.keys()), test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the captions\n",
        "all_captions = [caption for captions in captions_dict.values() for caption in captions]\n",
        "\n",
        "print(f\"Number of captions: {len(all_captions)}\")\n",
        "\n",
        "# Fit the tokenizer on the captions\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "# Define the vocabulary size and maximum caption length\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Maximum caption length: {max_length}\")\n",
        "\n",
        "# Encode the captions\n",
        "def encode_captions(captions):\n",
        "    sequences = tokenizer.texts_to_sequences(captions)\n",
        "    return pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Create dictionaries to map images to their encoded captions\n",
        "train_captions = {img: encode_captions(captions_dict[img]) for img in train_images}\n",
        "val_captions = {img: encode_captions(captions_dict[img]) for img in val_images}\n",
        "\n",
        "print(f\"Number of training images: {len(train_captions)}\")\n",
        "print(f\"Number of validation images: {len(val_captions)}\")\n"
      ],
      "metadata": {
        "id": "vZPYF-AaDRxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using EfficientNet for feature extraction\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
        "model_extract = Model(inputs=base_model.input, outputs=base_model.layers[-1].output)\n",
        "\n",
        "# function to extract features from an image\n",
        "def extract_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    features = model_extract.predict(img, verbose=0)\n",
        "    return features\n",
        "\n",
        "# Extract features for the training and validation images\n",
        "train_features = {img: extract_features(os.path.join('./flickr8k/Images', img)) for img in train_images}\n",
        "val_features = {img: extract_features(os.path.join('./flickr8k/Images', img)) for img in val_images}\n",
        "\n",
        "# save the features\n",
        "np.save('train_features.npy', train_features)\n",
        "np.save('val_features.npy', val_features)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yX9jMntHDvvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DistilBERT model and tokenizer\n",
        "distil_bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distil_bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def distil_bert_encode(texts):\n",
        "    inputs = distil_bert_tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=max_length)\n",
        "    outputs = distil_bert_model(inputs, training=False)\n",
        "    return outputs.last_hidden_state\n",
        "\n",
        "train_distil_bert_features = {img: distil_bert_encode(captions_dict[img]) for img in train_images}\n",
        "val_distil_bert_features = {img: distil_bert_encode(captions_dict[img]) for img in val_images}\n",
        "\n",
        "# Display the shapes of the extracted features\n",
        "print(train_distil_bert_features[train_images[0]].shape)\n",
        "print(val_distil_bert_features[val_images[0]].shape)\n",
        "\n",
        "# save the features\n",
        "np.save('train_distil_bert_features.npy', train_distil_bert_features)\n",
        "np.save('val_distil_bert_features.npy', val_distil_bert_features)\n"
      ],
      "metadata": {
        "id": "Cs5FsnSqDv7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bahdanau attention\n",
        "class BahdanauAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "mQk2Wf-DDRzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the features\n",
        "train_features = np.load('train_features.npy', allow_pickle=True).item()\n",
        "val_features = np.load('val_features.npy', allow_pickle=True).item()\n",
        "\n",
        "# Load the features\n",
        "train_distil_bert_features = np.load('train_distil_bert_features.npy', allow_pickle=True).item()\n",
        "val_distil_bert_features = np.load('val_distil_bert_features.npy', allow_pickle=True).item()\n",
        "\n",
        "\n",
        "# Model with a more complex combination of image and text features\n",
        "def build_model(vocab_size, max_length):\n",
        "    units = 256\n",
        "    image_input = Input(shape=(train_features[train_images[0]].shape[1],))\n",
        "    img_features = Dense(units, activation='relu')(image_input)\n",
        "    img_features = tf.expand_dims(img_features, 1)\n",
        "    img_features = Dropout(0.5)(img_features)\n",
        "\n",
        "    text_input = Input(shape=(max_length,))\n",
        "    text_features = Embedding(vocab_size, units)(text_input)\n",
        "    text_features = LSTM(units, return_sequences=True)(text_features)\n",
        "    text_features = Dropout(0.5)(text_features)\n",
        "\n",
        "    attention_layer = BahdanauAttention(units)\n",
        "    context_vector, _ = attention_layer(img_features, text_features[:, -1, :])\n",
        "\n",
        "    combined_features = Concatenate()([context_vector, text_features[:, -1, :]])\n",
        "\n",
        "    output = Dense(vocab_size, activation='softmax')(combined_features)\n",
        "\n",
        "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "model = build_model(vocab_size, max_length)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "fcrFN5rjDR2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "history = History()\n",
        "\n",
        "# Data generator\n",
        "def data_generator(features, captions, batch_size):\n",
        "    n = len(captions)\n",
        "    while True:\n",
        "        for i in range(0, n, batch_size):\n",
        "            batch_images = list(features.keys())[i:i+batch_size]\n",
        "            batch_features = [features[img][0] for img in batch_images]\n",
        "            batch_captions = [captions[img] for img in batch_images]\n",
        "            x1, x2, y = [], [], []\n",
        "            for j, img_features in enumerate(batch_features):\n",
        "                for caption in batch_captions[j]:\n",
        "                    seq = caption\n",
        "                    for k in range(1, len(seq)):\n",
        "                        in_seq, out_seq = seq[:k], seq[k]\n",
        "                        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                        out_seq = out_seq\n",
        "                        x1.append(img_features)\n",
        "                        x2.append(in_seq)\n",
        "                        y.append(out_seq)\n",
        "            yield [np.array(x1), np.array(x2)], np.array(y)\n",
        "\n",
        "# Training the model with more epochs, early stopping, and learning rate scheduling\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "steps_per_epoch = len(train_captions) // batch_size\n",
        "validation_steps = len(val_captions) // batch_size\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, min_lr=1e-7)\n",
        "history_callback = History()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    data_generator(train_features, train_captions, batch_size),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=data_generator(val_features, val_captions, batch_size),\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[early_stopping, lr_scheduler, history_callback]\n",
        ")\n",
        "\n",
        "# Plot training and validation loss\n",
        "def plot_loss(history):\n",
        "    plt.plot(history.history['loss'], label='train_loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(history)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o-3ID54dDR4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beam search implementation for caption generation using the trained model\n",
        "\n",
        "def beam_search(image_features, beam_width=3):\n",
        "    # Start with the start token\n",
        "    start = [tokenizer.word_index['startseq']]\n",
        "    start_word = [[start, 0.0]]\n",
        "\n",
        "    # Start the loop to perform Beam Search\n",
        "    while len(start_word[0][0]) < max_length:\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            sequence = pad_sequences([s[0]], maxlen=max_length)\n",
        "            preds = model.predict([image_features, sequence], verbose=0)\n",
        "            word_preds = np.argsort(preds[0])[-beam_width:]\n",
        "\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "\n",
        "        start_word = sorted(temp, reverse=False, key=lambda l: l[1])\n",
        "        start_word = start_word[-beam_width:]\n",
        "\n",
        "    final_caption = start_word[-1][0]\n",
        "    final_caption = [tokenizer.index_word[i] for i in final_caption if i > 0]\n",
        "    return ' '.join(final_caption[1:-1])\n",
        "\n",
        "\n",
        "# Above function for generating captions using beam search is good but the is\n",
        "\n"
      ],
      "metadata": {
        "id": "L4sz8jhGDR7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making a caption for a single image\n",
        "def make_caption(image_path):\n",
        "    img = extract_features(image_path)\n",
        "    caption_beam_search = beam_search(img, beam_width=7)\n",
        "    # caption = generate_caption(model, tokenizer, img, max_length)\n",
        "    original_caption = captions[captions['image'] == image_path.split('/')[-1]]['caption'].values[0]\n",
        "    # return [caption_beam_search, original_caption]\n",
        "    return [caption_beam_search, original_caption]\n",
        "\n",
        "# Display the image and caption\n",
        "def display_image_caption(image_path):\n",
        "    img = plt.imread(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    captions = make_caption(image_path)\n",
        "    print(f\"Caption using Beam Search: {captions[0]}, original caption: {captions[1]} /n bleu score: {sentence_bleu(captions[0], captions[1])}\")\n",
        "\n",
        "# display_image_caption('./flickr30k/Images/1000092795.jpg')\n",
        "# display_image_caption('./flickr30k/Images/10002456.jpg')\n",
        "# display_image_caption('./flickr30k/Images/1001545525.jpg')\n",
        "# display_image_caption('./archive/Images/115684808_cb01227802.jpg')\n",
        "# display_image_caption('./archive/Images/1178705300_c224d9a4f1.jpg')\n",
        "# display_image_caption('./archive/Images/2040941056_7f5fd50794.jpg')\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-QDW4rDR-x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}